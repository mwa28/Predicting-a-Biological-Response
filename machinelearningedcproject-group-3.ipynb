{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-28T21:38:34.183539Z","iopub.execute_input":"2022-05-28T21:38:34.184031Z","iopub.status.idle":"2022-05-28T21:38:34.194428Z","shell.execute_reply.started":"2022-05-28T21:38:34.184Z","shell.execute_reply":"2022-05-28T21:38:34.193462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/bioresponse/train.csv')\ntest = pd.read_csv('../input/bioresponse/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:37:54.800177Z","iopub.execute_input":"2022-05-28T21:37:54.800748Z","iopub.status.idle":"2022-05-28T21:37:56.570646Z","shell.execute_reply.started":"2022-05-28T21:37:54.800717Z","shell.execute_reply":"2022-05-28T21:37:56.569667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's start by checking the number of columns","metadata":{}},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:37:59.339152Z","iopub.execute_input":"2022-05-28T21:37:59.339514Z","iopub.status.idle":"2022-05-28T21:37:59.35104Z","shell.execute_reply.started":"2022-05-28T21:37:59.339486Z","shell.execute_reply":"2022-05-28T21:37:59.349662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_full = train.loc[:, train.columns!='Activity']\ny_full = train['Activity']","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:38:01.732965Z","iopub.execute_input":"2022-05-28T21:38:01.733371Z","iopub.status.idle":"2022-05-28T21:38:01.759491Z","shell.execute_reply.started":"2022-05-28T21:38:01.733334Z","shell.execute_reply":"2022-05-28T21:38:01.757792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(x_full, y_full, test_size=0.33, random_state=0)\nprint(X_train.shape)\nprint(X_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:38:04.296474Z","iopub.execute_input":"2022-05-28T21:38:04.297359Z","iopub.status.idle":"2022-05-28T21:38:05.755554Z","shell.execute_reply.started":"2022-05-28T21:38:04.297317Z","shell.execute_reply":"2022-05-28T21:38:05.754368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Given the large number of features, we need to do some feature selection. We will be using the PCA component analysis to select the features with the highest variance.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca_test = PCA(n_components=1776)\npca_test.fit(X_train)\nplt.plot(np.cumsum(pca_test.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.axvline(linewidth=4, color='r', linestyle = '--', x=324, ymin=0, ymax=1)\nplt.show()\nevr = pca_test.explained_variance_ratio_\ncvr = np.cumsum(pca_test.explained_variance_ratio_)\npca_df = pd.DataFrame()\npca_df['Cumulative Variance Ratio'] = cvr\npca_df['Explained Variance Ratio'] = evr","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:39:13.456766Z","iopub.execute_input":"2022-05-28T21:39:13.457454Z","iopub.status.idle":"2022-05-28T21:39:16.187194Z","shell.execute_reply.started":"2022-05-28T21:39:13.457419Z","shell.execute_reply":"2022-05-28T21:39:16.186194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_df.loc[320:350]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:39:23.044864Z","iopub.execute_input":"2022-05-28T21:39:23.045431Z","iopub.status.idle":"2022-05-28T21:39:23.064234Z","shell.execute_reply.started":"2022-05-28T21:39:23.045402Z","shell.execute_reply":"2022-05-28T21:39:23.063345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=0.95)\nX_train = pca.fit_transform(X_train)\nX_val = pca.transform(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:39:25.79071Z","iopub.execute_input":"2022-05-28T21:39:25.791764Z","iopub.status.idle":"2022-05-28T21:39:28.456364Z","shell.execute_reply.started":"2022-05-28T21:39:25.791731Z","shell.execute_reply":"2022-05-28T21:39:28.455372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.explained_variance_ratio_","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:39:29.906532Z","iopub.execute_input":"2022-05-28T21:39:29.906951Z","iopub.status.idle":"2022-05-28T21:39:29.91912Z","shell.execute_reply.started":"2022-05-28T21:39:29.906921Z","shell.execute_reply":"2022-05-28T21:39:29.917987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nclf = make_pipeline(StandardScaler(), SVC())\nclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:39:47.534887Z","iopub.execute_input":"2022-05-28T21:39:47.535278Z","iopub.status.idle":"2022-05-28T21:39:48.554005Z","shell.execute_reply.started":"2022-05-28T21:39:47.53525Z","shell.execute_reply":"2022-05-28T21:39:48.553439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ny_pred = clf.predict(X_val)\n\nprint(classification_report(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:39:50.637452Z","iopub.execute_input":"2022-05-28T21:39:50.637999Z","iopub.status.idle":"2022-05-28T21:39:51.322596Z","shell.execute_reply.started":"2022-05-28T21:39:50.637968Z","shell.execute_reply":"2022-05-28T21:39:51.321486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We are clearly not going far with this type of model. Let's tune it...","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n \n# defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']}\n \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n \n# fitting the model for grid search\ngrid.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:39:56.17496Z","iopub.execute_input":"2022-05-28T21:39:56.175368Z","iopub.status.idle":"2022-05-28T21:41:32.391892Z","shell.execute_reply.started":"2022-05-28T21:39:56.175339Z","shell.execute_reply":"2022-05-28T21:41:32.390939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid.best_params_)\n \n# print how our model looks after hyper-parameter tuning\nprint(grid.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:42:53.641689Z","iopub.execute_input":"2022-05-28T21:42:53.642203Z","iopub.status.idle":"2022-05-28T21:42:53.648184Z","shell.execute_reply.started":"2022-05-28T21:42:53.642175Z","shell.execute_reply":"2022-05-28T21:42:53.646789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_predictions = grid.predict(X_val)\n \n# print classification report\nprint(classification_report(y_val, grid_predictions))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:42:55.621754Z","iopub.execute_input":"2022-05-28T21:42:55.622874Z","iopub.status.idle":"2022-05-28T21:42:56.384495Z","shell.execute_reply.started":"2022-05-28T21:42:55.622809Z","shell.execute_reply":"2022-05-28T21:42:56.383154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clearly the model is not fit for this task...","metadata":{}}]}